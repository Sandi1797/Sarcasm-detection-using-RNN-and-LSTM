{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_acc = pd.read_json('Sarcasm_Headlines_Dataset.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar_acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \\\n",
       "0  former versace store clerk sues over secret 'b...             0   \n",
       "1  the 'roseanne' revival catches up to our thorn...             0   \n",
       "2  mom starting to fear son's web series closest ...             1   \n",
       "3  boehner just wants wife to listen, not come up...             1   \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
       "\n",
       "           source  \n",
       "0  huffingtonpost  \n",
       "1  huffingtonpost  \n",
       "2        theonion  \n",
       "3        theonion  \n",
       "4  huffingtonpost  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sar_acc['source'] = sar_acc['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\n",
    "sar_acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "label+percent",
         "labels": [
          "Acclaim",
          "Sarcastic"
         ],
         "marker": {
          "colors": [
           "#58D68D",
           "#9B59B6"
          ],
          "line": {
           "color": "#FFFFFF",
           "width": 2
          }
         },
         "opacity": 0.8,
         "type": "pie",
         "uid": "2c08805f-6ddf-4ab6-bcdd-1ec154b5fd83",
         "values": [
          56.10468381444457,
          43.89531618555543
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Sarcastic Vs Acclaim"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"0ec5b247-8753-49a2-840a-0912f851881e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"0ec5b247-8753-49a2-840a-0912f851881e\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '0ec5b247-8753-49a2-840a-0912f851881e',\n",
       "                        [{\"hoverinfo\": \"label+percent\", \"labels\": [\"Acclaim\", \"Sarcastic\"], \"marker\": {\"colors\": [\"#58D68D\", \"#9B59B6\"], \"line\": {\"color\": \"#FFFFFF\", \"width\": 2}}, \"opacity\": 0.8, \"type\": \"pie\", \"uid\": \"b70a8189-bc93-46f5-874b-5ee1f332046b\", \"values\": [56.10468381444457, 43.89531618555543]}],\n",
       "                        {\"title\": {\"text\": \"Sarcastic Vs Acclaim\"}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0ec5b247-8753-49a2-840a-0912f851881e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "sar_acc_tar = sar_acc['is_sarcastic'].value_counts()\n",
    "labels = ['Acclaim', 'Sarcastic']\n",
    "sizes = (np.array((sar_acc_tar / sar_acc_tar.sum())*100))\n",
    "colors = ['#58D68D', '#9B59B6']\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes, opacity = 0.8, hoverinfo='label+percent',\n",
    "               marker=dict(colors=colors, line=dict(color='#FFFFFF', width=2)))\n",
    "layout = go.Layout(\n",
    "    title='Sarcastic Vs Acclaim'\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename=\"Sa_Ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           5177,
           4170,
           3297,
           2961,
           2371,
           1885,
           1797,
           1666,
           1485,
           1229,
           1221,
           1189,
           1157,
           1051,
           914,
           885,
           882,
           838,
           812,
           802,
           773,
           770,
           753,
           731,
           723,
           699,
           648,
           648,
           630,
           613,
           578,
           570,
           564,
           559,
           551,
           531,
           524,
           513,
           489,
           489,
           487,
           484,
           468,
           466,
           450,
           438,
           420,
           410,
           391,
           389,
           388,
           387,
           385,
           384,
           382,
           369,
           368,
           345,
           341,
           336,
           334,
           326,
           325,
           323,
           320,
           320,
           319,
           314,
           312,
           310,
           301,
           294,
           290,
           288,
           286,
           284,
           281,
           279,
           276,
           271,
           265,
           257,
           257,
           254,
           253,
           249,
           245,
           244,
           244,
           238,
           235,
           231,
           230,
           229,
           228,
           228,
           227,
           226
          ],
          "colorscale": "Viridis"
         },
         "text": "Word counts",
         "type": "bar",
         "uid": "2f7a7916-84f8-4cab-a047-5816993a055e",
         "x": [
          "the",
          "in",
          "for",
          "a",
          "on",
          "and",
          "with",
          "is",
          "new",
          "man",
          "from",
          "at",
          "trump",
          "about",
          "you",
          "by",
          "this",
          "after",
          "be",
          "how",
          "out",
          "as",
          "that",
          "up",
          "it",
          "not",
          "your",
          "are",
          "his",
          "what",
          "he",
          "just",
          "who",
          "has",
          "will",
          "more",
          "all",
          "into",
          "have",
          "why",
          "one",
          "area",
          "donald",
          "over",
          "says",
          "can",
          "woman",
          "u.s."
         ],
         "y": [
          5177,
          4170,
          3297,
          2961,
          2371,
          1885,
          1797,
          1666,
          1485,
          1229,
          1221,
          1189,
          1157,
          1051,
          914,
          885,
          882,
          838,
          812,
          802,
          773,
          770,
          753,
          731,
          723,
          699,
          648,
          648,
          630,
          613,
          578,
          570,
          564,
          559,
          551,
          531,
          524,
          513,
          489,
          489,
          487,
          484,
          468,
          466,
          450,
          438,
          420,
          410
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Frequent Occuring word (unclean) in Headlines"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"5e532eff-29e0-4b81-8747-165caca088b2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"5e532eff-29e0-4b81-8747-165caca088b2\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '5e532eff-29e0-4b81-8747-165caca088b2',\n",
       "                        [{\"marker\": {\"color\": [5177, 4170, 3297, 2961, 2371, 1885, 1797, 1666, 1485, 1229, 1221, 1189, 1157, 1051, 914, 885, 882, 838, 812, 802, 773, 770, 753, 731, 723, 699, 648, 648, 630, 613, 578, 570, 564, 559, 551, 531, 524, 513, 489, 489, 487, 484, 468, 466, 450, 438, 420, 410, 391, 389, 388, 387, 385, 384, 382, 369, 368, 345, 341, 336, 334, 326, 325, 323, 320, 320, 319, 314, 312, 310, 301, 294, 290, 288, 286, 284, 281, 279, 276, 271, 265, 257, 257, 254, 253, 249, 245, 244, 244, 238, 235, 231, 230, 229, 228, 228, 227, 226], \"colorscale\": \"Viridis\"}, \"text\": \"Word counts\", \"type\": \"bar\", \"uid\": \"8219bef6-acc9-450f-b827-a2f19f976eca\", \"x\": [\"the\", \"in\", \"for\", \"a\", \"on\", \"and\", \"with\", \"is\", \"new\", \"man\", \"from\", \"at\", \"trump\", \"about\", \"you\", \"by\", \"this\", \"after\", \"be\", \"how\", \"out\", \"as\", \"that\", \"up\", \"it\", \"not\", \"your\", \"are\", \"his\", \"what\", \"he\", \"just\", \"who\", \"has\", \"will\", \"more\", \"all\", \"into\", \"have\", \"why\", \"one\", \"area\", \"donald\", \"over\", \"says\", \"can\", \"woman\", \"u.s.\"], \"y\": [5177, 4170, 3297, 2961, 2371, 1885, 1797, 1666, 1485, 1229, 1221, 1189, 1157, 1051, 914, 885, 882, 838, 812, 802, 773, 770, 753, 731, 723, 699, 648, 648, 630, 613, 578, 570, 564, 559, 551, 531, 524, 513, 489, 489, 487, 484, 468, 466, 450, 438, 420, 410]}],\n",
       "                        {\"title\": {\"text\": \"Frequent Occuring word (unclean) in Headlines\"}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5e532eff-29e0-4b81-8747-165caca088b2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words = sar_acc['headline'].str.split(expand=True).unstack().value_counts()\n",
    "data = [go.Bar(\n",
    "            x = all_words.index.values[2:50],\n",
    "            y = all_words.values[2:50],\n",
    "            marker= dict(colorscale='Viridis',\n",
    "                         color = all_words.values[2:100]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Frequent Occuring word (unclean) in Headlines'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_det = sar_acc[sar_acc.is_sarcastic==1]\n",
    "sar_det.reset_index(drop=True, inplace=True)\n",
    "acc_det = sar_acc[sar_acc.is_sarcastic==0]\n",
    "acc_det.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Tokenizing the Headlines of Sarcasm\n",
    "sar_news = []\n",
    "for rows in range(0, sar_det.shape[0]):\n",
    "    head_txt = sar_det.headline[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    sar_news.append(head_txt)\n",
    "\n",
    "#Converting into single list for Sarcasm\n",
    "import itertools\n",
    "sar_list = list(itertools.chain(*sar_news))\n",
    "\n",
    "# Tokenizing the Headlines of Acclaim\n",
    "acc_news = []\n",
    "for rows in range(0, acc_det.shape[0]):\n",
    "    head_txt = acc_det.headline[rows]\n",
    "    head_txt = head_txt.split(\" \")\n",
    "    acc_news.append(head_txt)\n",
    "    \n",
    "#Converting into single list for Acclaim\n",
    "acc_list = list(itertools.chain(*acc_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original Sarcasm list: 115898 words\n",
      "Length of Sarcasm list after stopwords removal: 87458 words\n",
      "============================================================================================\n",
      "Length of original Acclaim list: 147128 words\n",
      "Length of Acclaim list after stopwords removal: 103525 words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\n",
    "acc_list_restp = [word for word in acc_list if word.lower() not in stopwords]\n",
    "\n",
    "print(\"Length of original Sarcasm list: {0} words\\n\"\n",
    "      \"Length of Sarcasm list after stopwords removal: {1} words\"\n",
    "      .format(len(sar_list), len(sar_list_restp)))\n",
    "\n",
    "print(\"==\"*46)\n",
    "\n",
    "print(\"Length of original Acclaim list: {0} words\\n\"\n",
    "      \"Length of Acclaim list after stopwords removal: {1} words\"\n",
    "      .format(len(acc_list), len(acc_list_restp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning for getting top 30\n",
    "from collections import Counter\n",
    "sar_cnt = Counter(sar_list_restp)\n",
    "acc_cnt = Counter(acc_list_restp)\n",
    "\n",
    "#Dictonary to Dataframe\n",
    "sar_cnt_df = pd.DataFrame(list(sar_cnt.items()), columns = ['Words', 'Freq'])\n",
    "sar_cnt_df = sar_cnt_df.sort_values(by=['Freq'], ascending=False)\n",
    "acc_cnt_df = pd.DataFrame(list(acc_cnt.items()), columns = ['Words', 'Freq'])\n",
    "acc_cnt_df = acc_cnt_df.sort_values(by=['Freq'], ascending=False)\n",
    "\n",
    "#Top 30\n",
    "sar_cnt_df_30 = sar_cnt_df.head(30)\n",
    "acc_cnt_df_30 = acc_cnt_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "rgba(155, 89, 182, 0.6)",
          "line": {
           "color": "rgba(155, 89, 182, 1.0)",
           "width": 0.3
          }
         },
         "name": "Sarcasm",
         "opacity": 0.6,
         "orientation": "h",
         "type": "bar",
         "uid": "37fada90-8c0e-46aa-8e82-93d3f00a21c1",
         "x": [
          1021,
          821,
          477,
          360,
          290,
          253,
          213,
          208,
          207,
          200,
          193,
          187,
          187,
          187,
          173,
          171,
          169,
          169,
          166,
          157,
          157,
          152,
          147,
          146,
          144,
          142,
          133,
          131,
          130,
          126
         ],
         "xaxis": "x",
         "y": [
          "man",
          "new",
          "area",
          "report:",
          "woman",
          "one",
          "time",
          "still",
          "day",
          "trump",
          "nation",
          "get",
          "u.s.",
          "like",
          "finds",
          "back",
          "americans",
          "first",
          "family",
          "house",
          "obama",
          "life",
          "last",
          "going",
          "local",
          "nation's",
          "people",
          "white",
          "study",
          "little"
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "rgba(88, 214, 141, 0.6)",
          "line": {
           "color": "rgba(88, 214, 141, 1.0)",
           "width": 0.3
          }
         },
         "name": "Acclaim",
         "opacity": 0.6,
         "orientation": "h",
         "type": "bar",
         "uid": "09e199eb-056b-4218-9540-6a99b747102b",
         "x": [
          957,
          664,
          453,
          364,
          346,
          240,
          234,
          223,
          220,
          209,
          208,
          208,
          198,
          195,
          192,
          184,
          183,
          182,
          179,
          170,
          170,
          169,
          169,
          168,
          168,
          166,
          162,
          162,
          162,
          161
         ],
         "xaxis": "x2",
         "y": [
          "trump",
          "new",
          "donald",
          "trump's",
          "says",
          "women",
          "one",
          "u.s.",
          "first",
          "make",
          "people",
          "man",
          "get",
          "like",
          "gop",
          "day",
          "could",
          "5",
          "black",
          "need",
          "white",
          "clinton",
          "world",
          "obama",
          "police",
          "here's",
          "health",
          "life",
          "house",
          "bill"
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Top 30 Most occuring words in Sarcasm Headlines",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Top 30 Most occuring words in Acclaim Headlines",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          0.375
         ]
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"f35a6588-d23f-4c88-87ff-815a689e045c\" class=\"plotly-graph-div\" style=\"height:1200px; width:800px;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"f35a6588-d23f-4c88-87ff-815a689e045c\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'f35a6588-d23f-4c88-87ff-815a689e045c',\n",
       "                        [{\"marker\": {\"color\": \"rgba(155, 89, 182, 0.6)\", \"line\": {\"color\": \"rgba(155, 89, 182, 1.0)\", \"width\": 0.3}}, \"name\": \"Sarcasm\", \"opacity\": 0.6, \"orientation\": \"h\", \"type\": \"bar\", \"uid\": \"5da19ab7-99a0-4d90-9617-cbeb389c712b\", \"x\": [1021, 821, 477, 360, 290, 253, 213, 208, 207, 200, 193, 187, 187, 187, 173, 171, 169, 169, 166, 157, 157, 152, 147, 146, 144, 142, 133, 131, 130, 126], \"xaxis\": \"x\", \"y\": [\"man\", \"new\", \"area\", \"report:\", \"woman\", \"one\", \"time\", \"still\", \"day\", \"trump\", \"nation\", \"get\", \"u.s.\", \"like\", \"finds\", \"back\", \"americans\", \"first\", \"family\", \"house\", \"obama\", \"life\", \"last\", \"going\", \"local\", \"nation's\", \"people\", \"white\", \"study\", \"little\"], \"yaxis\": \"y\"}, {\"marker\": {\"color\": \"rgba(88, 214, 141, 0.6)\", \"line\": {\"color\": \"rgba(88, 214, 141, 1.0)\", \"width\": 0.3}}, \"name\": \"Acclaim\", \"opacity\": 0.6, \"orientation\": \"h\", \"type\": \"bar\", \"uid\": \"67176be1-8821-44b8-8205-1b8dd4a72820\", \"x\": [957, 664, 453, 364, 346, 240, 234, 223, 220, 209, 208, 208, 198, 195, 192, 184, 183, 182, 179, 170, 170, 169, 169, 168, 168, 166, 162, 162, 162, 161], \"xaxis\": \"x2\", \"y\": [\"trump\", \"new\", \"donald\", \"trump's\", \"says\", \"women\", \"one\", \"u.s.\", \"first\", \"make\", \"people\", \"man\", \"get\", \"like\", \"gop\", \"day\", \"could\", \"5\", \"black\", \"need\", \"white\", \"clinton\", \"world\", \"obama\", \"police\", \"here's\", \"health\", \"life\", \"house\", \"bill\"], \"yaxis\": \"y2\"}],\n",
       "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Top 30 Most occuring words in Sarcasm Headlines\", \"x\": 0.5, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Top 30 Most occuring words in Acclaim Headlines\", \"x\": 0.5, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 0.375, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"height\": 1200, \"width\": 800, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.625, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 0.375]}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f35a6588-d23f-4c88-87ff-815a689e045c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the top 30 Sarcasm Vs Acclaim\n",
    "from plotly import tools\n",
    "sar_tr  = go.Bar(\n",
    "    x=sar_cnt_df_30['Freq'],\n",
    "    y=sar_cnt_df_30['Words'],\n",
    "    name='Sarcasm',\n",
    "    marker=dict(\n",
    "        color='rgba(155, 89, 182, 0.6)',\n",
    "        line=dict(\n",
    "            color='rgba(155, 89, 182, 1.0)',\n",
    "            width=.3,\n",
    "        )\n",
    "    ),\n",
    "    orientation='h',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "acc_tr  = go.Bar(\n",
    "    x=acc_cnt_df_30['Freq'],\n",
    "    y=acc_cnt_df_30['Words'],\n",
    "    name='Acclaim',\n",
    "    marker=dict(\n",
    "        color='rgba(88, 214, 141, 0.6)',\n",
    "        line=dict(\n",
    "            color='rgba(88, 214, 141, 1.0)',\n",
    "            width=.3,\n",
    "        )\n",
    "    ),\n",
    "    orientation='h',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "fig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in Sarcasm Headlines',\n",
    "                                                          'Top 30 Most occuring words in Acclaim Headlines'))\n",
    "\n",
    "fig.append_trace(sar_tr, 1, 1)\n",
    "fig.append_trace(acc_tr, 2, 1)\n",
    "\n",
    "\n",
    "fig['layout'].update(height=1200, width=800)\n",
    "\n",
    "iplot(fig, filename='sar_vs_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of learning is: learn\n",
      "The stemmed form of learns is: learn\n",
      "The stemmed form of learn is: learn\n",
      "============================================================================================\n",
      "The stemmed form of leaves is: leav\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "print(\"The stemmed form of learning is: {}\".format(stemmer.stem(\"learning\")))\n",
    "print(\"The stemmed form of learns is: {}\".format(stemmer.stem(\"learns\")))\n",
    "print(\"The stemmed form of learn is: {}\".format(stemmer.stem(\"learn\")))\n",
    "print(\"==\"*46)\n",
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))\n",
    "print(\"==\"*46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized form of leaves is: leaf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarcasm headline after Lemmatization\n",
    "sar_wost_lem = []\n",
    "for batch in sar_news:\n",
    "    sar_list_restp = [word for word in batch if word.lower() not in stopwords]\n",
    "    lemm = WordNetLemmatizer()\n",
    "    sar_list_lemm =  [lemm.lemmatize(word) for word in sar_list_restp]\n",
    "    sar_wost_lem.append(sar_list_lemm)\n",
    "\n",
    "#Acclaim headline after Lemmatization\n",
    "acc_wost_lem = []\n",
    "for batch in acc_news:\n",
    "    acc_list_restp = [word for word in batch if word.lower() not in stopwords]\n",
    "    lemm = WordNetLemmatizer()\n",
    "    acc_list_lemm =  [lemm.lemmatize(word) for word in acc_list_restp]\n",
    "    acc_wost_lem.append(sar_list_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\\n\\nThe bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features are:\n",
      " ['christmas', 'francis', 'got', 'pope', 'sweater', 'vestment', 'wearing']\n",
      "\n",
      "The vectorized array looks like:\n",
      " [[0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = []\n",
    "for block in sar_wost_lem:\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "    sentence_transform = vectorizer.fit_transform(block)\n",
    "    vec.append(sentence_transform)\n",
    "    \n",
    "print(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\n",
    "print(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "#9B59B6"
         },
         "orientation": "h",
         "showlegend": false,
         "type": "bar",
         "uid": "b5ceb919-60fb-42cb-8d25-feeb0f665994",
         "x": [
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          18,
          18,
          19,
          20,
          20,
          21,
          22,
          22,
          23,
          23,
          24,
          28,
          30,
          31,
          31,
          32,
          32,
          33,
          36,
          36,
          37,
          40,
          42,
          60,
          91,
          103,
          223
         ],
         "xaxis": "x",
         "y": [
          "taylor swift",
          "get back",
          "bernie sander",
          "michelle obama",
          "secret service",
          "climate change",
          "man get",
          "feel like",
          "parking lot",
          "tv show",
          "john kerry",
          "tim kaine",
          "john kelly",
          "ted cruz",
          "average american",
          "jeb bush",
          "new line",
          "north korea",
          "report: american",
          "report: majority",
          "first time",
          "mike penny",
          "majority american",
          "new york",
          "look like",
          "poll find",
          "can't wait",
          "state union",
          "one day",
          "video game",
          "area dad",
          "local man",
          "paul ryan",
          "report find",
          "can't believe",
          "announces plan",
          "hillary clinton",
          "release new",
          "area man's",
          "historical archives:",
          "pope francis",
          "unveils new",
          "high school",
          "supreme court",
          "new study",
          "area woman",
          "introduces new",
          "white house",
          "study find",
          "area man"
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#58D68D"
         },
         "orientation": "h",
         "showlegend": false,
         "type": "bar",
         "uid": "40740b4b-6704-4b93-9f37-d76bb4d39188",
         "x": [
          14985,
          14985,
          14985,
          14985,
          14985,
          14985
         ],
         "xaxis": "x2",
         "y": [
          "pope francis",
          "francis wearing",
          "wearing sweater",
          "sweater vestment",
          "vestment got",
          "got christmas"
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Frequent bigrams of Sarcasm Headlines",
          "x": 0.2125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Frequent bigrams of Acclaim Headlines",
          "x": 0.7875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "paper_bgcolor": "rgb(233,233,233)",
        "title": {
         "text": "Bigram Plots Sarcasm Vs Acclaim after removing Stopwords"
        },
        "width": 900,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.425
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.575,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"2580fc9a-0ae1-469f-b46b-e970e943c94f\" class=\"plotly-graph-div\" style=\"height:1200px; width:900px;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"2580fc9a-0ae1-469f-b46b-e970e943c94f\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '2580fc9a-0ae1-469f-b46b-e970e943c94f',\n",
       "                        [{\"marker\": {\"color\": \"#9B59B6\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"uid\": \"7f191e54-dd8e-451d-a032-ecc8b46e7a4c\", \"x\": [13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 18, 18, 19, 20, 20, 21, 22, 22, 23, 23, 24, 28, 30, 31, 31, 32, 32, 33, 36, 36, 37, 40, 42, 60, 91, 103, 223], \"xaxis\": \"x\", \"y\": [\"taylor swift\", \"get back\", \"bernie sander\", \"michelle obama\", \"secret service\", \"climate change\", \"man get\", \"feel like\", \"parking lot\", \"tv show\", \"john kerry\", \"tim kaine\", \"john kelly\", \"ted cruz\", \"average american\", \"jeb bush\", \"new line\", \"north korea\", \"report: american\", \"report: majority\", \"first time\", \"mike penny\", \"majority american\", \"new york\", \"look like\", \"poll find\", \"can't wait\", \"state union\", \"one day\", \"video game\", \"area dad\", \"local man\", \"paul ryan\", \"report find\", \"can't believe\", \"announces plan\", \"hillary clinton\", \"release new\", \"area man's\", \"historical archives:\", \"pope francis\", \"unveils new\", \"high school\", \"supreme court\", \"new study\", \"area woman\", \"introduces new\", \"white house\", \"study find\", \"area man\"], \"yaxis\": \"y\"}, {\"marker\": {\"color\": \"#58D68D\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"uid\": \"e3a9a52a-1901-4bd2-84f6-84e22e529ead\", \"x\": [14985, 14985, 14985, 14985, 14985, 14985], \"xaxis\": \"x2\", \"y\": [\"pope francis\", \"francis wearing\", \"wearing sweater\", \"sweater vestment\", \"vestment got\", \"got christmas\"], \"yaxis\": \"y2\"}],\n",
       "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent bigrams of Sarcasm Headlines\", \"x\": 0.2125, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent bigrams of Acclaim Headlines\", \"x\": 0.7875, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"height\": 1200, \"paper_bgcolor\": \"rgb(233,233,233)\", \"title\": {\"text\": \"Bigram Plots Sarcasm Vs Acclaim after removing Stopwords\"}, \"width\": 900, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.425]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.575, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0]}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2580fc9a-0ae1-469f-b46b-e970e943c94f');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sar_wost_lem_df = pd.DataFrame({'sarcasm':sar_wost_lem})\n",
    "acc_wost_lem_df = pd.DataFrame({'acclaim':acc_wost_lem})\n",
    "\n",
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    ngrams = zip(*[text[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df, color):\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "#Plotting the Bigram plot\n",
    "from collections import defaultdict\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in sar_wost_lem_df[\"sarcasm\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "sar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n",
    "\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in acc_wost_lem_df[\"acclaim\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "acc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent bigrams of Sarcasm Headlines\", \n",
    "                                          \"Frequent bigrams of Acclaim Headlines\"])\n",
    "fig.append_trace(sar_2, 1, 1)\n",
    "fig.append_trace(acc_2, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\n",
    "iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "#9B59B6"
         },
         "orientation": "h",
         "showlegend": false,
         "type": "bar",
         "uid": "d26c2a63-1a09-4c83-b651-886845860285",
         "x": [
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          33
         ],
         "xaxis": "x",
         "y": [
          "first day school",
          "climate change study",
          "george r.r. martin",
          "british royal family",
          "study find average",
          "new york time",
          "taylor swift dating",
          "area man could",
          "area man think",
          "come crawling back",
          "fda approves new",
          "historical archives: sold",
          "archives: sold -",
          "area man idea",
          "donald trump jr.",
          "barnes & noble",
          "world war ii",
          "around white house",
          "ready go case",
          "find average american",
          "state union address",
          "area man pretty",
          "study find earth's",
          "'new york times'",
          "guy one show",
          "can't wait get",
          "high school student",
          "unveils new line",
          "new poll find",
          "supreme court justice",
          "new video game",
          "report: majority american",
          "world wildlife fund",
          "dream one day",
          "introduces new line",
          "study find majority",
          "study find american",
          "study find human",
          "new evidence suggests",
          "death row inmate",
          "report: average american",
          "new report find",
          "'no way prevent",
          "way prevent this,'",
          "prevent this,' say",
          "this,' say nation",
          "say nation regularly",
          "nation regularly happens",
          "secret service agent",
          "new study find"
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#58D68D"
         },
         "orientation": "h",
         "showlegend": false,
         "type": "bar",
         "uid": "49dea307-687f-4858-93cd-b45771e9e495",
         "x": [
          14985,
          14985,
          14985,
          14985,
          14985
         ],
         "xaxis": "x2",
         "y": [
          "pope francis wearing",
          "francis wearing sweater",
          "wearing sweater vestment",
          "sweater vestment got",
          "vestment got christmas"
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Frequent Trigrams of Sarcasm Headlines",
          "x": 0.2125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Frequent Trigrams of Acclaim Headlines",
          "x": 0.7875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 1200,
        "paper_bgcolor": "rgb(233,233,233)",
        "title": {
         "text": "Trigram Plots Sarcasm Vs Acclaim after removing Stopwords"
        },
        "width": 900,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.425
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.575,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"65bdc990-1a61-447b-8048-0e52eb1dfcde\" class=\"plotly-graph-div\" style=\"height:1200px; width:900px;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"65bdc990-1a61-447b-8048-0e52eb1dfcde\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '65bdc990-1a61-447b-8048-0e52eb1dfcde',\n",
       "                        [{\"marker\": {\"color\": \"#9B59B6\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"uid\": \"5d38b250-910e-464d-ba5e-251ea197a684\", \"x\": [3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 33], \"xaxis\": \"x\", \"y\": [\"first day school\", \"climate change study\", \"george r.r. martin\", \"british royal family\", \"study find average\", \"new york time\", \"taylor swift dating\", \"area man could\", \"area man think\", \"come crawling back\", \"fda approves new\", \"historical archives: sold\", \"archives: sold -\", \"area man idea\", \"donald trump jr.\", \"barnes & noble\", \"world war ii\", \"around white house\", \"ready go case\", \"find average american\", \"state union address\", \"area man pretty\", \"study find earth's\", \"'new york times'\", \"guy one show\", \"can't wait get\", \"high school student\", \"unveils new line\", \"new poll find\", \"supreme court justice\", \"new video game\", \"report: majority american\", \"world wildlife fund\", \"dream one day\", \"introduces new line\", \"study find majority\", \"study find american\", \"study find human\", \"new evidence suggests\", \"death row inmate\", \"report: average american\", \"new report find\", \"'no way prevent\", \"way prevent this,'\", \"prevent this,' say\", \"this,' say nation\", \"say nation regularly\", \"nation regularly happens\", \"secret service agent\", \"new study find\"], \"yaxis\": \"y\"}, {\"marker\": {\"color\": \"#58D68D\"}, \"orientation\": \"h\", \"showlegend\": false, \"type\": \"bar\", \"uid\": \"fd0c7371-ba48-45d0-a192-c506fa9a406c\", \"x\": [14985, 14985, 14985, 14985, 14985], \"xaxis\": \"x2\", \"y\": [\"pope francis wearing\", \"francis wearing sweater\", \"wearing sweater vestment\", \"sweater vestment got\", \"vestment got christmas\"], \"yaxis\": \"y2\"}],\n",
       "                        {\"annotations\": [{\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent Trigrams of Sarcasm Headlines\", \"x\": 0.2125, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 16}, \"showarrow\": false, \"text\": \"Frequent Trigrams of Acclaim Headlines\", \"x\": 0.7875, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1.0, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"height\": 1200, \"paper_bgcolor\": \"rgb(233,233,233)\", \"title\": {\"text\": \"Trigram Plots Sarcasm Vs Acclaim after removing Stopwords\"}, \"width\": 900, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 0.425]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.575, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 1.0]}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('65bdc990-1a61-447b-8048-0e52eb1dfcde');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the Trigram plot\n",
    "from collections import defaultdict\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in sar_wost_lem_df[\"sarcasm\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "sar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n",
    "\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in acc_wost_lem_df[\"acclaim\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "acc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent Trigrams of Sarcasm Headlines\", \n",
    "                                          \"Frequent Trigrams of Acclaim Headlines\"])\n",
    "fig.append_trace(sar_2, 1, 1)\n",
    "fig.append_trace(acc_2, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\n",
    "iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting X and Y ready\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = sar_acc.headline\n",
    "Y = sar_acc.is_sarcastic\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16826 samples, validate on 1870 samples\n",
      "Epoch 1/5\n",
      "16826/16826 [==============================] - 248s 15ms/step - loss: 0.3724 - acc: 0.8397 - val_loss: 0.4076 - val_acc: 0.8214\n",
      "Epoch 2/5\n",
      "16826/16826 [==============================] - 207s 12ms/step - loss: 0.3406 - acc: 0.8475 - val_loss: 0.4069 - val_acc: 0.8225\n",
      "Epoch 3/5\n",
      "16826/16826 [==============================] - 204s 12ms/step - loss: 0.3327 - acc: 0.8511 - val_loss: 0.3929 - val_acc: 0.8241\n",
      "Epoch 4/5\n",
      "16826/16826 [==============================] - 205s 12ms/step - loss: 0.3229 - acc: 0.8552 - val_loss: 0.4247 - val_acc: 0.8166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xf52f4d07f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "model.fit(sequences_matrix,Y_train,batch_size=100,epochs=5,\n",
    "          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8013/8013 [==============================] - 44s 5ms/step\n",
      "Test set\n",
      "  Loss: 0.418\n",
      "  Accuracy: 0.810\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
